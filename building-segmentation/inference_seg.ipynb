{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.utils import load_yaml, rle_encode\n",
    "from modules.model import get_model\n",
    "\n",
    "from modules.dataset import CustomDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import albumentations as A\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from transformers import (\n",
    "    AutoImageProcessor\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prj_dir = './'\n",
    "config_path = os.path.join(prj_dir, 'config', 'predict.yaml')\n",
    "config = load_yaml(config_path)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_transfrom = A.Compose([\n",
    "    A.Resize(config['input_size'], config['input_size'])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(f\"data/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = []\n",
    "for img in df[\"img_path\"]:\n",
    "    path = f\"./data/test_img/{os.path.basename(img)}\"\n",
    "    images.append(path)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#images = sorted(glob(f\"{config['data_dir']}/*.png\"))\n",
    "\n",
    "print('test len:', len(images))\n",
    "\n",
    "processor = AutoImageProcessor.from_pretrained(config['processor'])\n",
    "\n",
    "test_dataset = CustomDataset(processor=processor, \n",
    "                             images=images, \n",
    "                             masks=None, \n",
    "                             transform=test_transfrom, \n",
    "                             infer=True)\n",
    "\n",
    "test_dataloader = DataLoader(test_dataset, \n",
    "                              batch_size=config['batch_size'], \n",
    "                              num_workers=config['num_workers'],\n",
    "                              shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model(name=config['model']['name'])\n",
    "model = model.from_pretrained(\n",
    "    config['model']['weight'],\n",
    "    num_labels=1,\n",
    "    ignore_mismatched_sizes=True\n",
    ").to(device)\n",
    "\n",
    "weights = torch.load(config['model']['pretrained'])\n",
    "model.load_state_dict(weights)\n",
    "\n",
    "print('model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocess(outputs):\n",
    "    predicts = nn.functional.interpolate(\n",
    "        outputs.logits,\n",
    "        size=(config['input_size'], config['input_size']),\n",
    "        mode=\"bilinear\",\n",
    "        align_corners=False,\n",
    "    )\n",
    "\n",
    "    return predicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with torch.no_grad():\n",
    "#     model.eval()\n",
    "#     if config['amp']:\n",
    "#         for idx, (batch, filenames) in enumerate(tqdm(test_dataloader)):\n",
    "#             images = batch[\"pixel_values\"].to(device)\n",
    "#             outputs = model(images)\n",
    "#             predicts = postprocess(outputs)\n",
    "\n",
    "#             seg_prob = torch.sigmoid(predicts).detach().cpu().numpy().squeeze()\n",
    "#             seg = (seg_prob > 0.5).astype(np.uint8)\n",
    "#             break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    if config['amp']:\n",
    "        for idx, (batch, filenames) in enumerate(tqdm(test_dataloader)):\n",
    "            images = batch[\"pixel_values\"].to(device)\n",
    "            outputs = model(images)\n",
    "            predicts = postprocess(outputs)\n",
    "            \n",
    "            seg_prob = torch.sigmoid(predicts).detach().cpu().numpy().squeeze()\n",
    "            seg = (seg_prob > 0.5).astype(np.uint8)\n",
    "            \n",
    "            for i in range(len(images)):\n",
    "                mask_rle = rle_encode(seg[i])\n",
    "                if mask_rle == '':\n",
    "                    result.append(-1)\n",
    "                else:\n",
    "                    result.append(mask_rle)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit = pd.read_csv('./data/sample_submission.csv')\n",
    "submit['mask_rle'] = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit.to_csv('./submit2.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
